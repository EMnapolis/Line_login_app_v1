# utility_ai.py
import json
import os
import requests
import subprocess
from utility_chat import *

# Initialize SQLite connection
conn, cursor = init_db()
initialize_schema(conn)

# ===== API / Auth configuration =====
CHANNEL_ID = os.getenv("CHANNEL_ID")
CHANNEL_SECRET = os.getenv("CHANNEL_SECRET")
REDIRECT_URI = os.getenv("REDIRECT_URI")
STATE = os.getenv("STATE")
CHAT_TOKEN = os.getenv("CHAT_TOKEN")

# Logging configuration
ACCESS_LOG_FILE = os.getenv("ACCESS_LOG_FILE", "access_log.txt")

# AI Keys configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Ensure environment variable is set
OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL")


# ===== Function to count tokens =====
def count_tokens(text, model="gpt-4o"):
    """
    Counts the number of tokens in a given text for a specified model.
    """
    try:
        encoding = tiktoken.encoding_for_model(model)
    except Exception:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))


def estimate_tokens(text: str, model: str = None) -> int:
    """
    Estimates the number of tokens based on the number of words in the text.
    """
    words = len(text.split())
    return int(words / 0.75)


# ===== Function to parse LLaMA stream response =====
def parse_llama_stream_response(res):
    """
    Parse and decode a JSON stream response from LLaMA.
    """
    reply = ""
    raw_chunks = []
    decoder = json.JSONDecoder()

    try:
        for line in res.iter_lines():
            if line:
                try:
                    line_str = line.decode("utf-8").strip()
                    while line_str:
                        obj, idx = decoder.raw_decode(line_str)
                        raw_chunks.append(obj)
                        reply += obj.get("response", "")
                        line_str = line_str[idx:].lstrip()
                except Exception as e:
                    print("‚ùå Error decoding JSON chunk:", e)
                    continue
    except Exception as e:
        print("‚ùå Error reading response stream:", e)

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    if not reply.strip():
        reply = "‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ stream"
        raw_chunks.append(
            {
                "response": "",
                "error": "empty or invalid stream",
                "fallback_message": reply,
            }
        )

    return reply, {"chunks": raw_chunks, "full_reply": reply}


# ===== Function to stream responses based on model =====
def stream_response_by_model(model_name, messages, stream_output):
    """
    Streams responses from models like GPT (OpenAI) or Ollama based on the model name.
    """
    reply = ""
    raw_json = {}
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0

    # ===== GPT Models Response =====
    if model_name.startswith("gpt-"):
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=True,
        )

        chunks = []
        for chunk in response:
            if st.session_state.get("stop_chat", False):
                stream_output.markdown("üõë Stopped displaying.")
                break
            chunks.append(chunk.model_dump())
            if chunk.choices and chunk.choices[0].delta.content:
                word = chunk.choices[0].delta.content
                reply += word
                stream_output.markdown(reply + "‚ñå", unsafe_allow_html=True)

        if not reply.strip():
            reply = "‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á"
        stream_output.markdown(reply)
        st.session_state["stop_chat"] = False

        raw_json = {
            "model": model_name,
            "chunks": chunks,
            "full_reply": reply,
        }

        prompt_tokens = sum(
            count_tokens(m["content"], model=model_name) for m in messages
        )
        completion_tokens = count_tokens(reply, model=model_name)
        total_tokens = prompt_tokens + completion_tokens

    # ===== OLLAMA Models Response =====
    else:
        llama_server = os.getenv(
            "OLLAMA_SERVER_API", "http://localhost:11434/api/generate"
        )
        full_prompt = (
            "\n".join([f"{m['role']}: {m['content']}" for m in messages])
            + "\nassistant:"
        )

        try:
            res = requests.post(
                llama_server,
                json={"model": model_name, "prompt": full_prompt, "stream": True},
                stream=True,
            )

            # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            reply, raw_chunks = parse_llama_stream_response(res)

            if not reply.strip():
                reply = "‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏• Ollama ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å stream"
                raw_chunks["chunks"].append({"warning": "empty response"})

            stream_output.markdown(reply)
            st.session_state["stop_chat"] = False

            raw_json = {
                "model": model_name,
                "chunks": raw_chunks["chunks"],
                "full_reply": reply,
            }

            prompt_tokens = estimate_tokens(full_prompt)
            completion_tokens = estimate_tokens(reply)
            total_tokens = prompt_tokens + completion_tokens

        except Exception as e:
            stream_output.markdown(f"‚ùå Unable to connect to Ollama: {e}")
            return {
                "reply": "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama ‡πÑ‡∏î‡πâ",
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
                "response_json": json.dumps({"error": str(e)}, ensure_ascii=False),
            }

    return {
        "reply": reply,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "response_json": json.dumps(raw_json, ensure_ascii=False),
    }


# ===== Function to get list of available Ollama models =====
def get_ollama_models():
    """
    Fetches the list of available models from the Ollama server.
    """
    try:
        response = requests.get(f"{OLLAMA_SERVER_URL}/api/tags")
        response.raise_for_status()
        data = response.json()
        return [m["name"] for m in data.get("models", [])]
    except Exception as e:
        print(f"‚ùå Unable to fetch model list from Ollama: {e}")
        return []


# ===== Display AI response information =====
def display_ai_response_info(model_choice, base_messages, stream_output):
    """
    Calls the model and displays its response along with token usage, model, and the time taken.
    """
    start_time = time.time()
    result = stream_response_by_model(model_choice, base_messages, stream_output)
    end_time = time.time()
    duration = round(end_time - start_time, 2)

    reply = result.get("reply", "").strip()

    # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
    if not reply:
        reply = "‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"
        stream_output.markdown(reply)
    else:
        stream_output.markdown(reply)

    st.caption(
        f"üìå Model used: `{model_choice}` | "
        f"Tokens: Prompt = {result['prompt_tokens']}, Completion = {result['completion_tokens']}, "
        f"Total = {result['total_tokens']} | "
        f"‚è±Ô∏è Time taken: {duration} seconds"
    )

    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà update ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    result["reply"] = reply
    return result


# ===== Function to check token quota =====
def check_token_quota():
    """
    Checks the user's token usage and warns if nearing the limit.
    """
    from utility_chat import init_db

    conn, cursor = init_db()
    current_user = st.session_state.get("user_id", "")
    role = st.session_state.get("role", "").lower()

    # ‚úÖ ‡∏î‡∏∂‡∏á display_name ‡∏à‡∏≤‡∏Å session ‡∏´‡∏£‡∏∑‡∏≠ access_login
    display_name = st.session_state.get("displayName", "")
    if not display_name:
        cursor.execute(
            "SELECT display_name FROM access_login WHERE user_id = ?", (current_user,)
        )
        row = cursor.fetchone()
        display_name = row[0] if row and row[0] else current_user

    # üî¢ ‡∏î‡∏∂‡∏á token usage ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
    cursor.execute(
        """
        SELECT SUM(total_tokens) FROM token_usage
        WHERE user_id = ?
        """,
        (current_user,),
    )
    used_token = cursor.fetchone()[0] or 0

    # üìè ‡∏î‡∏∂‡∏á quota override ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    cursor.execute(
        """
        SELECT quota_override FROM token_usage
        WHERE user_id = ? AND quota_override IS NOT NULL
        ORDER BY id DESC LIMIT 1
        """,
        (current_user,),
    )
    quota_row = cursor.fetchone()
    quota_limit = quota_row[0] if quota_row else 1_000_000  # Default fallback quota

    # üéØ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
    percent_used = round((used_token / quota_limit) * 100, 2)

    # ‚ö†Ô∏è ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏¥‡∏ô 90%
    if percent_used >= 90 and used_token < quota_limit:
        st.warning(
            f"‚ö†Ô∏è ‡∏Ñ‡∏∏‡∏ì `{display_name}` ‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ Token ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß {percent_used}%\n"
            f"üî¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏õ: `{used_token:,}` / `{quota_limit:,}` tokens\n"
            f"üß≠ ‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
        )

    # üõë ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô quota
    if used_token >= quota_limit and role not in ["admin", "super admin"]:
        st.error(
            f"‚ùå ‡∏Ñ‡∏∏‡∏ì `{display_name}` ‡πÉ‡∏ä‡πâ Token ‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÅ‡∏•‡πâ‡∏ß\n"
            f"üî¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏õ: `{used_token:,}` / `{quota_limit:,}` tokens\n"
            f"üõë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≠‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
        )
        st.stop()

def plot_grouped_bar(df, group_col, category_col):
    import plotly.express as px
    import streamlit as st

    try:
        grouped_df = (
            df[[group_col, category_col]]
            .dropna()
            .groupby([group_col, category_col])
            .size()
            .reset_index(name="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")
        )

        fig = px.bar(
            grouped_df,
            x=group_col,
            y="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô",
            color=category_col,
            barmode="group",
            text="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô",
            title=f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô '{category_col}' ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ '{group_col}'",
            height=500,
        )
        fig.update_layout(xaxis_title=group_col, yaxis_title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")
        st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
