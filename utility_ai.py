# utility_ai.py
from utility_chat import *
import requests
import subprocess

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ SQLite
conn, cursor = init_db()
initialize_schema(conn)
# ==== AI KEY ====

# API / Auth
CHANNEL_ID = os.getenv("CHANNEL_ID")
CHANNEL_SECRET = os.getenv("CHANNEL_SECRET")
REDIRECT_URI = os.getenv("REDIRECT_URI")
STATE = os.getenv("STATE")
CHAT_TOKEN = os.getenv("CHAT_TOKEN")

# Logging
ACCESS_LOG_FILE = os.getenv("ACCESS_LOG_FILE", "access_log.txt")

# AI Keys
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL")

# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡∏ö Token ==========
def count_tokens(text, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except Exception:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def estimate_tokens(text: str, model: str = None) -> int:
    words = len(text.split())
    return int(words / 0.75)


# ========== ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å stream ‡∏Ç‡∏≠‡∏á LLaMA ==========
def parse_llama_stream_response(res):
    import json

    reply = ""
    raw_chunks = []
    decoder = json.JSONDecoder()

    for line in res.iter_lines():
        if line:
            try:
                line_str = line.decode("utf-8").strip()

                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ JSON object ‡∏ï‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
                while line_str:
                    obj, idx = decoder.raw_decode(line_str)
                    raw_chunks.append(obj)
                    reply += obj.get("response", "")
                    line_str = line_str[idx:].lstrip()

            except Exception as e:
                print("‚ùå Error decoding JSON chunk:", e)
                continue

    return reply, {"chunks": raw_chunks, "full_reply": reply}


# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠ ==========
def stream_response_by_model(model_name, messages, stream_output):
    """
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö GPT (OpenAI), Ollama (LLaMA/Mistral/etc.), ‡πÅ‡∏•‡∏∞ fallback
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ dict ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞ token usage
    """
    import json, os, requests
    from openai import OpenAI

    reply = ""
    raw_json = {}
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0

    # ========== GPT MODELS ==========
    if model_name.startswith("gpt-"):
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=True,
        )

        chunks = []
        for chunk in response:
            if st.session_state.get("stop_chat", False):
                stream_output.markdown("üõë ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß")
                break
            chunks.append(chunk.model_dump())
            if chunk.choices and chunk.choices[0].delta.content:
                word = chunk.choices[0].delta.content
                reply += word
                stream_output.markdown(reply + "‚ñå", unsafe_allow_html=True)

        stream_output.markdown(reply)
        st.session_state["stop_chat"] = False

        raw_json = {
            "model": "gpt-4o",
            "chunks": chunks,
            "full_reply": reply,
        }

        prompt_tokens = sum(
            count_tokens(m["content"], model=model_name) for m in messages
        )
        completion_tokens = count_tokens(reply, model=model_name)
        total_tokens = prompt_tokens + completion_tokens

    # ========== OLLAMA MODELS ==========
    elif True:  # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà GPT
        llama_server = os.getenv(
            "OLLAMA_SERVER_API", "http://localhost:11434/api/generate"
        )
        full_prompt = (
            "\n".join([f"{m['role']}: {m['content']}" for m in messages])
            + "\nassistant:"
        )

        try:
            res = requests.post(
                llama_server,
                json={"model": model_name, "prompt": full_prompt, "stream": True},
                stream=True,
            )

            reply = ""
            raw_chunks = []
            decoder = json.JSONDecoder()

            for line in res.iter_lines():
                if st.session_state.get("stop_chat", False):
                    stream_output.markdown("üõë ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß")
                    break
                if line:
                    try:
                        line_str = line.decode("utf-8").strip()
                        while line_str:
                            obj, idx = decoder.raw_decode(line_str)
                            raw_chunks.append(obj)
                            reply += obj.get("response", "")
                            line_str = line_str[idx:].lstrip()
                            stream_output.markdown(reply + "‚ñå", unsafe_allow_html=True)
                    except Exception as e:
                        print("‚ùå Error decoding JSON chunk:", e)

            stream_output.markdown(reply)
            st.session_state["stop_chat"] = False

            raw_json = {
                "model": model_name,
                "chunks": raw_chunks,
                "full_reply": reply,
            }

            prompt_tokens = estimate_tokens(full_prompt)
            completion_tokens = estimate_tokens(reply)
            total_tokens = prompt_tokens + completion_tokens

        except Exception as e:
            stream_output.markdown(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama ‡πÑ‡∏î‡πâ: {e}")
            return {
                "reply": "",
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
                "response_json": "{}",
            }

    return {
        "reply": reply,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "response_json": json.dumps(raw_json, ensure_ascii=False),
    }


def get_ollama_models():
    try:
        response = requests.get(f"{OLLAMA_SERVER_URL}/api/tags")
        response.raise_for_status()
        data = response.json()
        return [m["name"] for m in data.get("models", [])]
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å Ollama ‡πÑ‡∏î‡πâ: {e}")
        return []


def display_ai_response_info(model_choice, base_messages, stream_output):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å model ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• tokens, model ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ä‡πâ
    ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô result["reply"] ‡∏î‡πâ‡∏ß‡∏¢
    """
    start_time = time.time()

    result = stream_response_by_model(model_choice, base_messages, stream_output)

    end_time = time.time()
    duration = round(end_time - start_time, 2)

    reply = result["reply"]
    stream_output.markdown(reply)

    st.caption(
        f"üìå ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•: `{model_choice}` | "
        f"Tokens: Prompt = {result['prompt_tokens']}, Completion = {result['completion_tokens']}, "
        f"‡∏£‡∏ß‡∏° = {result['total_tokens']} | "
        f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {duration} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ"
    )

    return result


# üß† ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö token quota
def check_token_quota():
    from utility_chat import init_db

    conn, cursor = init_db()
    current_user = st.session_state.get("user_id", "")
    role = st.session_state.get("role", "").lower()

    # ‚úÖ ‡∏î‡∏∂‡∏á‡∏£‡∏ß‡∏° Token ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
    cursor.execute(
        """
        SELECT SUM(total_tokens) FROM token_usage
        WHERE user_id = ?
        """,
        (current_user,),
    )
    used_token = cursor.fetchone()[0] or 0

    # ‚úÖ ‡∏î‡∏∂‡∏á quota override ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å DB
    cursor.execute(
        """
        SELECT quota_override FROM token_usage
        WHERE user_id = ? AND quota_override IS NOT NULL
        ORDER BY id DESC LIMIT 1
        """,
        (current_user,),
    )
    quota_row = cursor.fetchone()
    quota_limit = quota_row[0] if quota_row else 1_000_000  # DEFAULT_QUOTA fallback

    # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß
    percent_used = round((used_token / quota_limit) * 100, 2)

    # ‚ö†Ô∏è ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏ô 90%
    if percent_used >= 90 and used_token < quota_limit:
        st.warning(
            f"""
            ‚ö†Ô∏è ‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ Token ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß {percent_used}%  
            üî¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏õ: `{used_token:,}` ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ `{quota_limit:,}` Tokens  
            üß≠ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            """
        )

    # ‚ùå ‡∏´‡∏¢‡∏∏‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤
    if used_token >= quota_limit and role not in ["admin", "super admin"]:
        st.error(
            f"""
            ‚ùå ‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ Token ‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏•‡πâ‡∏ß  
            üî¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏õ: `{used_token:,}` ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ `{quota_limit:,}` Tokens  
            üõë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≠‡∏£‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà
            """
        )
        st.stop()
