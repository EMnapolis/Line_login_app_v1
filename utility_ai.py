# utility_ai.py
from utility_chat import *
import requests

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ SQLite
conn, cursor = init_db()
initialize_schema(conn)


# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡∏ö Token ==========
def count_tokens(text, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except Exception:
        encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

# def attach_token_count(messages: list, model: str = "gpt-4o") -> list:
#     """
#     ‡πÄ‡∏û‡∏¥‡πà‡∏° field 'token_count' ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô messages
#     ‡πÉ‡∏ä‡πâ count_tokens ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPT ‡∏´‡∏£‡∏∑‡∏≠ estimate_tokens ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLaMA
#     """
#     for m in messages:
#         if "token_count" not in m and "content" in m:
#             if model.startswith("gpt-"):
#                 m["token_count"] = count_tokens(m["content"], model=model)
#             else:
#                 m["token_count"] = estimate_tokens(m["content"])
#     return messages

def estimate_tokens(text: str, model: str = None) -> int:
    words = len(text.split())
    return int(words / 0.75)


# ========== ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å stream ‡∏Ç‡∏≠‡∏á LLaMA ==========
def parse_llama_stream_response(res):
    reply = ""
    chunks = []

    for line in res.iter_lines():
        if line:
            chunk = json.loads(line.decode("utf-8"))
            chunks.append(chunk)
            if "response" in chunk:
                reply += chunk["response"]

    full_json = chunks[-1] if chunks and chunks[-1].get("done") else {}
    return reply, full_json


# ========== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠ ==========
def stream_response_by_model(model_name, messages, stream_output):
    """
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö GPT (OpenAI) ‡πÅ‡∏•‡∏∞ LLM Local (‡πÄ‡∏ä‡πà‡∏ô LLaMA ‡∏ú‡πà‡∏≤‡∏ô Ollama)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ dict ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞ token usage
    """
    import os
    import json
    from openai import OpenAI

    reply = ""
    raw_json = {}
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0

    if model_name.startswith("gpt-"):
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=True,
        )

        chunks = []  # üëà ‡πÄ‡∏Å‡πá‡∏ö raw chunk
        for chunk in response:
            chunks.append(chunk.model_dump())  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö raw JSON ‡∏ï‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
            if chunk.choices and chunk.choices[0].delta.content:
                word = chunk.choices[0].delta.content
                reply += word
                stream_output.markdown(reply + "‚ñå", unsafe_allow_html=True)

        stream_output.markdown(reply)

        raw_json = {
            "model": model_name,
            "chunks": chunks,  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏∏‡∏Å chunk
            "full_reply": reply,
        }

        # ‡∏ô‡∏±‡∏ö token (GPT ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
        from utility_ai import count_tokens

        prompt_tokens = sum(
            count_tokens(m["content"], model=model_name) for m in messages
        )
        completion_tokens = count_tokens(reply, model=model_name)
        total_tokens = prompt_tokens + completion_tokens

    elif model_name.startswith(("llama", "mistral", "phi")) or model_name.endswith(
        ":latest"
    ):
        from utility_ai import estimate_tokens, parse_llama_stream_response

        full_prompt = (
            "\n".join([f"{m['role']}: {m['content']}" for m in messages])
            + "\nassistant:"
        )

        res = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model_name, "prompt": full_prompt, "stream": True},
            stream=True,
        )

        reply, raw_json = parse_llama_stream_response(res)

        for i in range(0, len(reply), 10):
            stream_output.markdown(reply[: i + 10] + "‚ñå", unsafe_allow_html=True)
        stream_output.markdown(reply)

        prompt_tokens = estimate_tokens(full_prompt)
        completion_tokens = estimate_tokens(reply)
        total_tokens = prompt_tokens + completion_tokens

    else:
        stream_output.markdown("‚ùå ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ")
        return {
            "reply": "",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "response_json": "{}",
        }

    return {
        "reply": reply,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "response_json": json.dumps(raw_json, ensure_ascii=False),
    }
