# utility.py
import streamlit as st
import os
import sqlite3
import pandas as pd
from io import StringIO
from io import BytesIO
from openai import OpenAI
from datetime import datetime
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from charset_normalizer import from_bytes
from dotenv import load_dotenv
import tiktoken
import json
import time
import math
import requests
# ==== import libary ====

# ==== global path ====
db_folder = os.path.join("data")
db_path = os.path.join("data", "sqdata.db")
schema_path = os.path.join("data", "schema.sql")
user_id = st.session_state.get("user_id")
# ==== global path ====

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å config.py ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ os.getenv() ‡πÄ‡∏≠‡∏á
from config import OPENAI_API_KEY,CHAT_TOKEN

# ===== OpenAI Client =====
client = OpenAI(api_key=OPENAI_API_KEY)

# ===== ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQLite =====
def init_db():
    if not os.path.exists(db_folder):
        os.makedirs(db_folder)

    first_time = not os.path.exists(db_path)
    conn = sqlite3.connect(db_path, check_same_thread=False)
    
    if first_time:
        initialize_schema(conn)
        
    return conn, conn.cursor()

# ===== ‡πÇ‡∏´‡∏•‡∏î schema.sql ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á =====
def initialize_schema(conn, schema_path=schema_path):
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô SQL schema ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .sql ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
    """
    if not os.path.exists(schema_path):
        raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå schema: {schema_path}")

    with open(schema_path, "r", encoding="utf-8") as f:
        sql_script = f.read().strip()

    if not sql_script:
        raise ValueError("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå schema ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")

    try:
        conn.executescript(sql_script)
        conn.commit()
        print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
    except Exception as e:
        conn.rollback()
        raise RuntimeError(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á schema: {e}")


# ===== ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• =====
def save_conversation_if_ready(
    conn, cursor, messages_key, source="chat_gpt", **token_usage
):
    messages = st.session_state.get(messages_key, [])
    conv_key = f"conversation_id_{messages_key}"
    last_key = f"last_saved_count_{messages_key}"
    conv_id = st.session_state.get(conv_key)
    last_saved_count = st.session_state.get(last_key, 0)
    user_id = st.session_state.get("user_id", "guest")

    if len(messages) >= 2 and len(messages) > last_saved_count:
        last_two = messages[-2:]
        if last_two[0]["role"] == "user" and last_two[1]["role"] == "assistant":
            if conv_id is None:
                title = generate_title_from_conversation(messages)
                cursor.execute(
                    """
                    INSERT INTO conversations (
                        user_id, title, source,
                        prompt_tokens, completion_tokens, total_tokens
                    )
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (
                        user_id,
                        title,
                        source,
                        token_usage.get("prompt_tokens", 0),
                        token_usage.get("completion_tokens", 0),
                        token_usage.get("total_tokens", 0),
                    ),
                )
                conv_id = cursor.lastrowid
                st.session_state[conv_key] = conv_id

            for msg in messages[last_saved_count:]:
                cursor.execute(
                    """
                    INSERT INTO messages (
                        user_id, conversation_id, role, content,
                        prompt_tokens, completion_tokens, total_tokens
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        user_id,
                        conv_id,
                        msg.get("role", "user"),
                        msg.get("content", ""),
                        msg.get("prompt_tokens", 0),
                        msg.get("completion_tokens", 0),
                        msg.get("total_tokens", 0),
                    ),
                )
                message_id = cursor.lastrowid

                if "response_json" in msg:
                    cursor.execute(
                        """
                        INSERT INTO raw_json (conversation_id, message_id, response_json)
                        VALUES (?, ?, ?)
                        """,
                        (conv_id, message_id, msg["response_json"]),
                    )

            conn.commit()
            st.session_state[last_key] = len(messages)
            st.toast(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å {source}")


# ===== ‡πÉ‡∏ä‡πâ AI ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠ =====
def generate_title_from_conversation(messages):
    try:
        system_prompt = {"role": "system", "content": "You are an assistant that summarizes the topic of a conversation in 5-10 Thai words."}
        response = client.chat.completions.create(
            model="model_name",
            messages=[system_prompt] + messages + [{"role": "user", "content": "‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡∏±‡πâ‡∏ô ‡πÜ"}]
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return "‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà" if not messages else messages[0].get("content", "‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà")[:30]

# ===== ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ =====
def list_conversations(user_id=None):
    db_path = os.path.join("data", "sqdata.db")
    conn = sqlite3.connect(db_path, check_same_thread=False)

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° token columns
    query = """
        SELECT id, user_id, title, source,
               prompt_tokens, completion_tokens, total_tokens,
               created_at
        FROM conversations
    """
    params = ()

    if user_id:
        query += " WHERE user_id = ?"
        params = (user_id,)

    query += " ORDER BY created_at DESC"
    df = pd.read_sql_query(query, conn, params=params)
    conn.close()
    return df.values.tolist()

# ===== ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏£‡∏≤‡∏á Prompt =====
def init_prompt_table():
    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS prompts (
            prompt_name TEXT,
            user_id TEXT NOT NULL,
            content TEXT,
            prompt_tokens INTEGER,
            completion_tokens INTEGER,
            total_tokens INTEGER,
            PRIMARY KEY (prompt_name, user_id)
        )
    """)
    conn.commit()
def save_prompt(prompt_name, content):
    user_id = st.session_state.get("user_id")
    if not user_id:
        st.warning("‚õî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö")
        return
    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("""
        REPLACE INTO prompts (prompt_name, user_id, content, prompt_tokens, completion_tokens, total_tokens)
        VALUES (?, ?, ?, ?, ?, ?)
    """, (prompt_name, user_id, content, 0, 0, 0))  # ‡∏õ‡∏£‡∏±‡∏ö token = 0 ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö parameter ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    conn.commit()
    conn.close()
def list_prompts():
    user_id = st.session_state.get("user_id")
    role = st.session_state.get("Role", "user")

    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()

    if role == "admin":
        cursor.execute("SELECT prompt_name, content FROM prompts ORDER BY prompt_name")
    else:
        cursor.execute("SELECT prompt_name, content FROM prompts WHERE user_id = ? ORDER BY prompt_name", (user_id,))
    results = cursor.fetchall()
    conn.close()
    return results
def delete_prompt(name):
    user_id = st.session_state.get("user_id")
    role = st.session_state.get("Role", "user")

    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()

    if role == "admin":
        cursor.execute("DELETE FROM prompts WHERE prompt_name = ?", (name,))
    else:
        cursor.execute("DELETE FROM prompts WHERE prompt_name = ? AND user_id = ?", (name, user_id))

    conn.commit()
    conn.close()

## ============================== ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå ==============================
# ===== ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö encoding ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° =====
# ===== ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö encoding ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° =====
def try_decode_file(file_bytes: bytes) -> str:
    try:
        result = from_bytes(file_bytes).best()
        if result:
            return str(result)
    except Exception:
        pass
    try:
        return file_bytes.decode("utf-8")
    except UnicodeDecodeError:
        return file_bytes.decode("iso-8859-1", errors="replace")
# ===== ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain =====
def process_file_to_chain(uploaded_file):
    if not uploaded_file:
        st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô")
        return

    with st.spinner("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå..."):
        try:
            file_bytes = uploaded_file.read()
            file_content = try_decode_file(file_bytes)
            st.text_area("üìñ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤", file_content[:1000], height=200, disabled=True)

            doc = Document(page_content=file_content)
            splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)
            chunks = splitter.split_documents([doc])
            st.success(f"üìö ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô {len(chunks)} ‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢")

            embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
            vectorstore = Chroma.from_documents(chunks, embedding=embeddings)

            chain = ConversationalRetrievalChain.from_llm(
                llm=ChatOpenAI(model="model_name", temperature=0, openai_api_key=OPENAI_API_KEY),
                retriever=vectorstore.as_retriever(),
                memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True),
            )

            st.session_state["chain"] = chain
            st.session_state["chat_history"] = []
            st.session_state["file_content"] = file_content
            st.success("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")

        except Exception as e:
            st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
# ===== üîé ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î =====
def read_uploaded_file(file_name, uploaded_file, chunk_size=5000):
    file_name = file_name.lower()
    my_bar = st.progress(0, text="üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå...")
    uploaded_file.seek(0)

    if file_name.endswith(".xlsx"):
        df = pd.read_excel(uploaded_file)
        content = df.to_string(index=False)
    elif file_name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
        content = df.to_string(index=False)
    else:
        file_bytes = uploaded_file.read()
        result = from_bytes(file_bytes).best()
        content = str(result) if result else file_bytes.decode("utf-8")

    total_parts = math.ceil(len(content) / chunk_size)
    collected = []

    for i in range(total_parts):
        part = content[i * chunk_size : (i + 1) * chunk_size]
        collected.append(part)
        percent = int((i + 1) / total_parts * 100)
        my_bar.progress(percent, text=f"üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà {i+1}/{total_parts} ({percent}%)")
        time.sleep(0.01)

    my_bar.empty()
    return "".join(collected)
# ===== üí¨ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó =====
def append_chat(role, content, state_key="chat_history"):
    """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏á‡πÉ‡∏ô session chat"""
    st.chat_message(role).write(content)
    st.session_state.setdefault(state_key, []).append({
        "role": role,
        "content": content
    })
# ===== ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô Document ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM =====
def get_split_docs(uploaded_file, chunk_size=3000, chunk_overlap=200):
    """‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢ Document ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤"""
    file_name = uploaded_file.name.lower()

    # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
    if file_name.endswith(".xlsx"):
        df = pd.read_excel(uploaded_file)
        file_content = df.to_string(index=False)
    elif file_name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
        file_content = df.to_string(index=False)
    else:
        file_bytes = uploaded_file.read()
        result = from_bytes(file_bytes).best()
        if result is None:
            raise ValueError("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ")
        file_content = str(result)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Document ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡πà‡∏≠‡∏ô
    doc = Document(page_content=file_content)

    # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢ chunk
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks = splitter.split_documents([doc])
    total = len(all_chunks)

    # ‡πÅ‡∏™‡∏î‡∏á progress
    st.info(f"üìÑ ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô {total} ‡∏™‡πà‡∏ß‡∏ô ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")

    progress_bar = st.progress(0)
    processed_chunks = []
    for i, chunk in enumerate(all_chunks):
        processed_chunks.append(chunk)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        percent_complete = int((i + 1) / total * 100)
        progress_bar.progress(percent_complete)
        time.sleep(0.01)  # ‡∏à‡∏≥‡∏•‡∏≠‡∏á delay (‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏à‡∏£‡∏¥‡∏á)

    st.success("‚úÖ ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")

    return processed_chunks, file_content
# ===== ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å GPT ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (DataFrame) =====
def call_openai_with_parsing(full_input, system_prompt):
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)

    base_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": full_input}
    ]

    response = client.chat.completions.create(
        model="model_name",
        messages=base_messages
    )

    reply = response.choices[0].message.content
    usage = response.usage
    raw_json = json.dumps(response.model_dump(), ensure_ascii=False)

    df_result = None
    try:
        if "|" in reply and "---" in reply:
            lines = [line for line in reply.splitlines() if "|" in line and not line.strip().startswith("---")]
            cleaned = "\n".join(lines)
            df_result = pd.read_csv(StringIO(cleaned), sep="|").dropna(axis=1, how="all")
        elif "," in reply:
            df_result = pd.read_csv(StringIO(reply))
    except Exception:
        df_result = None

    return reply, df_result, usage, raw_json
# ===== üß† ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ Prompt ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å =====
def process_uploaded_file_for_prompt(uploaded_file):
    try:
        uploaded_file.seek(0)
        file_name = uploaded_file.name.lower()

        if file_name.endswith(".txt"):
            file_content = uploaded_file.read().decode("utf-8", errors="ignore")
        elif file_name.endswith(".csv"):
            df = pd.read_csv(uploaded_file)
            file_content = df.to_csv(index=False)
        elif file_name.endswith(".xlsx"):
            df = pd.read_excel(uploaded_file)
            file_content = df.to_csv(index=False)
        else:
            raise ValueError("‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")

        st.session_state["file_content"] = file_content
        st.session_state["analysis_results"] = []

        st.success("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
        st.text_area("üìÑ ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå", file_content[:3000], height=200, disabled=True)


    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {e}")
        st.stop()
# üì• ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå AI (txt / md)
def show_download_section():
    if st.session_state.get("show_download"):
        st.markdown("### üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")

        file_format = st.selectbox(
            "üìÑ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå", ["txt", "csv", "xlsx"], key="download_format"
        )

        file_name = st.text_input(
            "üìÅ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå", value="analysis_result", key="download_filename"
        )

        full_filename = f"{file_name.strip()}.{file_format}"
        file_bytes = BytesIO()

        # ‚ñ∂Ô∏è ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (csv, xlsx)
        if (
            file_format in ["csv", "xlsx"]
            and st.session_state.get("analysis_result_table") is not None
        ):
            raw_data = st.session_state["analysis_result_table"]

            # üß† ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
            if isinstance(raw_data, list):
                if all(isinstance(item, list) for item in raw_data):
                    # ‚õ≥Ô∏è list of list ‚Üí ‡πÅ‡∏¢‡∏Å header ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    if len(raw_data) >= 2:
                        header = raw_data[0]
                        rows = raw_data[1:]
                        df = pd.DataFrame(rows, columns=header)
                    else:
                        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á")
                        return
                elif all(isinstance(item, dict) for item in raw_data):
                    df = pd.DataFrame(raw_data)
                else:
                    st.warning(
                        "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô list of list ‡∏´‡∏£‡∏∑‡∏≠ list of dict)"
                    )
                    return
            else:
                st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÑ‡∏î‡πâ")
                return

            with st.expander("üîç ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á"):
                st.dataframe(df)

            if file_format == "csv":
                df.to_csv(file_bytes, index=False, encoding="utf-8-sig")
                mime_type = "text/csv"
            elif file_format == "xlsx":
                with pd.ExcelWriter(file_bytes, engine="xlsxwriter") as writer:
                    df.to_excel(writer, index=False, sheet_name="Result")
                mime_type = (
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

        else:
            # ‚ñ∂Ô∏è ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ (txt)
            content = st.session_state.get("analysis_result", "")
            if not content.strip():
                st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
                return

            with st.expander("üîç ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å"):
                st.text(content)
            file_bytes.write(content.encode("utf-8"))
            mime_type = "text/plain"

        file_bytes.seek(0)

        # ‚¨áÔ∏è ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        st.download_button(
            label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå",
            data=file_bytes,
            file_name=full_filename,
            mime=mime_type,
        )

## ============================== ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå ==============================