from utility_ai import *

st.title("ü§ñ ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡∏±‡∏ö AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ")

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_choice = st.radio(
    "üß† ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ", ["gpt-4o", "llama2:latest"], horizontal=True
)

# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
uploaded_file = st.file_uploader(
    "üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (txt, csv, xlsx)", type=["txt", "csv", "xlsx"]
)
if uploaded_file:
    try:
        file_content = read_uploaded_file(uploaded_file.name, uploaded_file)
        st.session_state["file_text"] = file_content
        st.text_area(
            "üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå", file_content[:1000], height=200, disabled=True
        )
    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {e}")
        st.stop()

# ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
file_content = st.session_state.get("file_text", "")
st.session_state.setdefault("chat_all_in_one", [])

# ‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
for msg in st.session_state["chat_all_in_one"]:
    st.chat_message(msg["role"]).write(msg["content"])

# ‡∏ä‡πà‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ß‡πà‡∏≤ '‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå')"):
    from utility_ai import count_tokens, estimate_tokens

    token_fn = count_tokens if model_choice.startswith("gpt-") else estimate_tokens

    st.chat_message("user").write(prompt)
    st.session_state["chat_all_in_one"].append(
        {
            "role": "user",
            "content": prompt,
            "token_count": token_fn(prompt, model_choice),
        }
    )

    if prompt.strip() == "‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå" or (
        "save" in prompt.lower() and st.session_state.get("analysis_result")
    ):
        st.chat_message("assistant").write("üì¶ ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å AI")
        st.session_state["show_download"] = True
    else:
        try:
            base_messages = [
                {
                    "role": "system",
                    "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏≤‡∏Å‡∏°‡∏µ",
                }
            ]
            if file_content:
                base_messages.append(
                    {"role": "user", "content": f"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:\n{file_content}"}
                )
            base_messages.extend(st.session_state["chat_all_in_one"])

            # ‡πÄ‡∏û‡∏¥‡πà‡∏° token ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
            # base_messages = attach_token_count(base_messages, model=model_choice)

            with st.chat_message("assistant"):
                stream_output = st.empty()
                result = stream_response_by_model(
                    model_choice, base_messages, stream_output
                )
                stream_output.markdown(result["reply"])

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ history
            st.session_state["chat_all_in_one"].append(
                {
                    "role": "assistant",
                    "content": result["reply"],
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["total_tokens"],
                    "response_json": result["response_json"],
                }
            )

            st.session_state["analysis_result"] = result["reply"]
            st.session_state["show_download"] = False

            st.session_state["messages_gpt"] = base_messages + [
                {
                    "role": "assistant",
                    "content": result["reply"],
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["total_tokens"],
                    "response_json": result["response_json"],
                }
            ]

            save_conversation_if_ready(
                conn,
                cursor,
                messages_key="messages_gpt",
                source=model_choice,  # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á
                prompt_tokens=result["prompt_tokens"],
                completion_tokens=result["completion_tokens"],
                total_tokens=result["total_tokens"],
            )

        except Exception as e:
            st.error(f"‚ùå Error: {e}")

# ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
show_download_section()
